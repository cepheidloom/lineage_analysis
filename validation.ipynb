{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a594dd",
   "metadata": {},
   "source": [
    "### Create set of all object present in json files inside dedicated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e758f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "folder = \"../lineage_outputs/\"\n",
    "json_list = []\n",
    "set_creation_error = dict()\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    with open(folder+file, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "        json_file[\"file_name\"] = file.replace(\".json\", \"\").split(\"--\")[1] + \".\" + file.replace(\".json\", \"\").split(\"--\")[2]\n",
    "\n",
    "        try:\n",
    "            src_trg_set = set()\n",
    "            for src_trg_pairs in json_file[\"lineage\"]:\n",
    "                src_trg_set.add(src_trg_pairs[\"source\"])\n",
    "                src_trg_set.add(src_trg_pairs[\"target\"])\n",
    "            src_trg_set.remove(json_file[\"file_name\"])\n",
    "        except Exception as e:\n",
    "            #if the name of error is simply the name of file then pass\n",
    "            if str(e).replace(\"'\",\"\") == json_file[\"file_name\"]:\n",
    "                pass\n",
    "            else:\n",
    "                set_creation_error[json_file[\"file_name\"]] = e\n",
    "            \n",
    "        src_trg_set = {element.lower() if element is not None else element for element in src_trg_set}\n",
    "\n",
    "        json_file[\"content_set\"] = src_trg_set\n",
    "        json_list.append(json_file)\n",
    "\n",
    "print(\"Errors encountered while creating set for each json file: \", len(set_creation_error))\n",
    "\n",
    "print(\"Key template of each dictionary in list: \",json_list[0].keys(), \"\\n\")\n",
    "print(json_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb8455",
   "metadata": {},
   "source": [
    "### Create set of all lineage objects present in csv file for each dependent object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# Load data\n",
    "df_dev_linage = pd.read_csv(\"databases_lineage_extracted.csv\")\n",
    "df_uat_linage = pd.read_csv(\"database_lineage_extracted_2.csv\")\n",
    "df_unioned = pd.concat([df_dev_linage, df_uat_linage], ignore_index=True)\n",
    "df_unioned\n",
    "\n",
    "subset_cols = ['Dependent_Schema', 'Dependent_Object_Name', 'Dependent_Object_Type',\n",
    "               'Depends_On_Schema', 'Depends_On_Object_Name', 'Depends_On_Object_Type']\n",
    "df_lineage_merged = df_unioned.drop_duplicates(subset=subset_cols)\n",
    "\n",
    "#--------------------------------- get lineage content set from excel file for each object ----------------------\n",
    "lineage_list = {}\n",
    "for (dependent_schema, dependent_object_name), group in df_lineage_merged.groupby(\n",
    "    [\"Dependent_Schema\", \"Dependent_Object_Name\"]\n",
    "):\n",
    "    dependent_object_name = re.sub(r\"[^\\w\\s-]\", \"\", dependent_object_name).replace(\n",
    "        \" \", \"_\"\n",
    "    )\n",
    "    lineage_list[dependent_schema + \".\" + dependent_object_name] = set()\n",
    "    for row in group.itertuples():\n",
    "        content = row[7] + \".\" + row[8]\n",
    "        lineage_list[dependent_schema + \".\" + dependent_object_name].add(\n",
    "            content.lower()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OBJECTS that are present in both the databases but with different lineage mappings\n",
    "\n",
    "# # 1. Define your group\n",
    "# grouped = df_lineage_merged.groupby(['Dependent_Schema', 'Dependent_Object_Name'])\n",
    "# # 2. Filter for groups that appear in more than one database\n",
    "# # We use nunique() in case a database is listed twice for some reason\n",
    "# df_multi_db = grouped.filter(lambda x: x['Database'].nunique() > 1)\n",
    "# # 3. Sort to see the duplicates side-by-side\n",
    "# df_multi_db = df_multi_db.sort_values(by=subset_cols)\n",
    "# list_different_lineage_mapping = df_multi_db[['Dependent_Schema', 'Dependent_Object_Name']].drop_duplicates().values.tolist()\n",
    "# # print(\"Total objects that have different lineage mappings in both lineage sheets:\\n\", len(list_different_lineage_mapping), \"\\n\")\n",
    "# # for element in list_different_lineage_mapping:\n",
    "# #     print(\".\".join(element))\n",
    "# # for row in df_multi_db.values.tolist():\n",
    "# #     print(row)\n",
    "# subset_cols_multi_db = subset_cols.copy()\n",
    "# subset_cols_multi_db.insert(0,\"Database\")\n",
    "# df_multi_db = df_multi_db[subset_cols_multi_db].sort_values(by=[\"Dependent_Schema\", \"Dependent_Object_Name\", \"Database\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf70d4b",
   "metadata": {},
   "source": [
    "### Analyze both data structure and find out unequal sets containing lineage objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa234dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "incorrect = []\n",
    "errors = []\n",
    "for json_item in json_list:\n",
    "    try:\n",
    "        if json_item[\"content_set\"] == lineage_list[json_item[\"file_name\"]]:\n",
    "            correct.append(json_item[\"file_name\"])\n",
    "        else:\n",
    "            incorrect.append(json_item[\"file_name\"])\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "\n",
    "incorrect.sort()\n",
    "correct.sort()\n",
    "\n",
    "#---------------------------------Create dict using above code variables and print in json format-----------------\n",
    "\n",
    "stats_dict = {\n",
    "    \"correct\": len(correct),\n",
    "    \"incorrect\": len(incorrect),\n",
    "    \"total_jsons_processed\": len(correct) + len(incorrect),\n",
    "    \"errors\": len(errors),\n",
    "    \"names\": {\n",
    "        \"correct_file_names\": correct,\n",
    "        \"incorrect_file_names\": incorrect,\n",
    "        \"errors_file_names\": [str(e) for e in errors],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(json.dumps(stats_dict,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd22d12",
   "metadata": {},
   "source": [
    "## Search content from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "#####################\n",
    "with open(\"target_object.yaml\", \"r\") as f:\n",
    "    f = yaml.safe_load(f)\n",
    "depend_schema = f['target_tables']['depend_schema']\n",
    "dependent_object_name = f['target_tables']['dependent_object_name']\n",
    "#####################\n",
    "\n",
    "print(depend_schema+'.'+dependent_object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for object in correct:\n",
    "    if object == depend_schema + '.' + dependent_object_name:\n",
    "        print(\"Object present in `CORRECT` list\")\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "for object in incorrect:\n",
    "    if object == depend_schema + '.' + dependent_object_name:\n",
    "        print(\"Object present in `INCORRECT` list\")\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find item and it's lineage set in json_list read from json files.\n",
    "for item in json_list:\n",
    "    if item['file_name'] == depend_schema + '.' + dependent_object_name:\n",
    "        print(f\"file_name: {item['file_name']}\\n\")\n",
    "        print(\"-------CONTENT PRESENT IN json_list-------\\n\")\n",
    "        print(\"\\n\".join(sorted(item['content_set'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lineage_list:\n",
    "    if item == depend_schema + '.' + dependent_object_name:\n",
    "        print('Dependent_object_name: ', item)\n",
    "        print(\"\\n-------CONTENT PRESENT IN lineage_list-------\\n\")\n",
    "        print(\"\\n\".join(sorted(lineage_list[item])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_linage = pd.read_csv(\"databases_lineage_extracted.csv\")\n",
    "df_uat_linage = pd.read_csv(\"database_lineage_extracted_2.csv\")\n",
    "df_unioned = pd.concat([df_dev_linage, df_uat_linage], ignore_index=True)\n",
    "df_unioned\n",
    "\n",
    "subset_cols = ['Dependent_Schema', 'Dependent_Object_Name', 'Dependent_Object_Type',\n",
    "               'Depends_On_Schema', 'Depends_On_Object_Name', 'Depends_On_Object_Type']\n",
    "df = df_unioned.drop_duplicates(subset=subset_cols)\n",
    "df = df.query(f\"Dependent_Schema == '{depend_schema}' and Dependent_Object_Name == '{dependent_object_name}'\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "def robust_clean_sql(sql_query):\n",
    "    sql_text = str(sql_query)\n",
    "\n",
    "    sql_text = sql_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "\n",
    "    # Remove single-line comments (-- ...)\n",
    "    sql_text = re.sub(r'--.*', '', sql_text)\n",
    "    # Remove multi-line comments (/* ... */)\n",
    "    sql_text = re.sub(r'/\\*.*?\\*/', '', sql_text, flags=re.DOTALL)\n",
    "\n",
    "    # Replace multiple newlines with a single newline\n",
    "    sql_text = re.sub(r'\\n\\s*\\n', '\\n', sql_text)\n",
    "    # Collapse horizontal spaces (tabs/spaces) into one space\n",
    "    sql_text = re.sub(r'[ \\t]+', ' ', sql_text)\n",
    "    \n",
    "    return sql_text.strip()\n",
    "\n",
    "df_definitions = pd.concat([pd.read_csv(\"object_definitions.csv\"), pd.read_csv(\"UAT_object_definitions.csv\")], ignore_index=True)\n",
    "\n",
    "df_definitions = df_definitions.query(\"ObjectType == 'SQL_STORED_PROCEDURE'\")\n",
    "# # 1. Sort so that preferred database comes first\n",
    "df_definitions = df_definitions.sort_values(by=\"DatabaseName\", ascending=False)\n",
    "df_definitions = df_definitions.drop_duplicates(subset=['Schema', 'Object'], keep='first')\n",
    "# # #get all objects present in multiple databases\n",
    "# # df_duplicates = df_definitions.groupby(['Schema', 'Object']).filter(lambda x: len(x) >= 2)\n",
    "# counts = df_definitions['DatabaseName'].value_counts()\n",
    "# print(counts)\n",
    "\n",
    "#get count of all different schemas present in unioned dataframe of object_definitions CSVs\n",
    "schema_counts = df_definitions.query(\"ObjectType == 'SQL_STORED_PROCEDURE'\")['Schema'].value_counts()\n",
    "schema_counts\n",
    "\n",
    "# df_definitions = df_definitions.query(f\"Schema == '{depend_schema}' and Object == '{dependent_object_name}'\")\n",
    "# df_definitions\n",
    "\n",
    "# 1. Filter, Group, and Count\n",
    "df_schema_counts = (\n",
    "    df_definitions\n",
    "    .query(\"ObjectType == 'SQL_STORED_PROCEDURE'\")  # Filter data\n",
    "    .groupby('Schema')                               # Group by Schema\n",
    "    .size()                                          # Count rows per group\n",
    "    .reset_index(name='Count')                       # Convert to DataFrame & name column\n",
    ")\n",
    "\n",
    "# 2. Sort by count (Optional, usually helpful for Excel)\n",
    "df_schema_counts = df_schema_counts.sort_values(by='Count', ascending=False).reset_index()\n",
    "df_schema_counts\n",
    "# small_schemas_list = []\n",
    "# for row in df_schema_counts.query(\"Count<4\").itertuples():\n",
    "#     small_schemas_list.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877fe008",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(small_schemas_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50542d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_definitions.iloc[0,4])\n",
    "print(robust_clean_sql(df_definitions.iloc[0,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30e253",
   "metadata": {},
   "source": [
    "<!-- # schema_counts = df_definitions.query(\"ObjectType == 'SQL_STORED_PROCEDURE'\")['Schema'].value_counts()\n",
    "# schema_counts -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b957d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_files = set()\n",
    "for file in os.listdir(\"lineage_outputs\"):\n",
    "    obj_name = file.split(\"--\")[1] + file.split(\"--\")[2]\n",
    "    if obj_name in dir_files:\n",
    "        print(obj_name)\n",
    "    dir_files.add(obj_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4efc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dir_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
