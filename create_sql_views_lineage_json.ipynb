{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load data\n",
    "df_dev_linage = pd.read_csv(\"local_files/database_lineage_extracted_1.csv\")\n",
    "df_uat_linage = pd.read_csv(\"local_files/database_lineage_extracted_2.csv\")\n",
    "df_unioned = pd.concat([df_dev_linage, df_uat_linage], ignore_index=True)\n",
    "df_unioned\n",
    "\n",
    "df_unioned = df_unioned[df_unioned[\"Dependent_Object_Type\"] == 'VIEW'].copy()\n",
    "\n",
    "subset_cols = ['Dependent_Schema', 'Dependent_Object_Name', 'Dependent_Object_Type',\n",
    "               'Depends_On_Schema', 'Depends_On_Object_Name', 'Depends_On_Object_Type']\n",
    "df = df_unioned.drop_duplicates(subset=subset_cols)\n",
    "df = df.copy()\n",
    "# #Check unique views\n",
    "# counts = df.groupby(['Dependent_Schema', 'Dependent_Object_Name']).size().reset_index(name='Occurrence_Count')\n",
    "# counts\n",
    "# Group by the schema and object name\n",
    "# Filter for groups where the number of UNIQUE databases is greater than 1\n",
    "# result = df.groupby(['Dependent_Schema', 'Dependent_Object_Name']).filter(\n",
    "#     lambda x: x['Database'].nunique() > 1\n",
    "# )\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309076c3-8340-47aa-8adb-5dfba021eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# 1. Setup the dedicated folder\n",
    "folder_name = \"lineage_outputs\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "\n",
    "# 2. Create concatenated full names for JSON content only (Schema.ObjectName)\n",
    "df.loc[:, 'Target_Full'] = df['Dependent_Schema'] + \".\" + df['Dependent_Object_Name']\n",
    "df.loc[:, 'Source_Full'] = df['Depends_On_Schema'] + \".\" + df['Depends_On_Object_Name']\n",
    "\n",
    "# 3. Group and Export\n",
    "# Enumerate starts at 1 for the index\n",
    "for idx, (target_full, group) in enumerate(df.groupby('Target_Full'), 1):\n",
    "    \n",
    "    dependencies = []\n",
    "    # Get schema and object name from the first row of the group for naming\n",
    "    # We grab these directly to avoid parsing the '.' later\n",
    "    schema_name = group['Dependent_Schema'].iloc[0]\n",
    "    object_name = group['Dependent_Object_Name'].iloc[0]\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        dependencies.append({\n",
    "            \"source\": row['Source_Full'],\n",
    "            \"target\": row['Target_Full']\n",
    "        })\n",
    "    \n",
    "    # SANITIZATION:\n",
    "    # Remove illegal characters from schema and object name individually\n",
    "    safe_schema = re.sub(r'[<>:\"/\\\\|?*]', '_', str(schema_name))\n",
    "    safe_object = re.sub(r'[<>:\"/\\\\|?*]', '_', str(object_name))\n",
    "    \n",
    "    separator = \"--\" \n",
    "    \n",
    "    filename = f\"{idx}{separator}{safe_schema}{separator}{safe_object}.json\"\n",
    "    \n",
    "    file_path = os.path.join(folder_name, filename)\n",
    "    \n",
    "    # Wrap dependencies in a \"lineage\" key\n",
    "    output_data = {\n",
    "        \"lineage\": dependencies\n",
    "    }\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "        \n",
    "    # print(f\"Generated: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
